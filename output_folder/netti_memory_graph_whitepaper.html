<html>
 <body>
  <p>
   SNL-1: White Paper
  </p>
  <p>
   Title
  </p>
  <p>
   From Tokens to Thought: Inside the Netti-AI Memory Graph
  </p>
  <p>
   Author
  </p>
  <p>
   SynaptechLabs
  </p>
  <p>
   Date
  </p>
  <p>
   June 2025
  </p>
  <p>
   Abstract
  </p>
  <p>
   Netti-AI is a biologically inspired neural reasoning engine designed to simulate human-like memory, context
  </p>
  <p>
   awareness,  and  cognitive  association.  At  its  core  lies  the  Netti  Memory  Graph-a  symbolic,  weighted,  and
  </p>
  <p>
   dynamic structure representing the relationship between concepts over time. This white paper dives into the
  </p>
  <p>
   internal workings of the memory graph, exploring how tokenized input is transformed into structured cognition
  </p>
  <p>
   through activation, recall, and emotional bias.
  </p>
  <p>
   1. Introduction
  </p>
  <p>
   Traditional neural networks operate on fixed architectures and numeric gradients. Netti-AI takes a symbolic
  </p>
  <p>
   approach  where  individual  neurons  (or  'nodes')  represent  discrete  tokens  tagged  by  type,  mood,  role,  or
  </p>
  <p>
   context.  Connections  form  through  co-activation,  evolving  a  graph  of  meaningful  associations  that  can  be
  </p>
  <p>
   recalled, strengthened, or pruned.
  </p>
  <p>
   Netti's architecture seeks to mirror the flexible, emergent, and recursive nature of thought-not just processing
  </p>
  <p>
   information, but growing understanding.
  </p>
  <p>
   2. Tokenization and Input Flow
  </p>
  <p>
   All input to Netti-AI is first passed through a Tokenizer, which parses and tags the data:
  </p>
  <p>
   - word:home
  </p>
  <p>
   - num:3.14
  </p>
  <p>
   SNL-1: White Paper
  </p>
  <p>
   - mood:curious
  </p>
  <p>
   - punc:?
  </p>
  <p>
   These  tokens  are  then  injected  into  the  neural  graph  as  activation  pulses.  Their  tags  guide  link  formation,
  </p>
  <p>
   filtering, and neuron creation.
  </p>
  <p>
   3. The Memory Graph
  </p>
  <p>
   The Netti Memory Graph consists of:
  </p>
  <p>
   - Nodes: Represent symbols (e.g., concepts, emotions, syntax units).
  </p>
  <p>
   - Links: Directed, weighted, inhibitory or excitatory.
  </p>
  <p>
   - Activation States: Transient values representing the current focus of thought.
  </p>
  <p>
   - Decay Functions: Prevent runaway activation and enforce forgetting.
  </p>
  <p>
   Nodes are linked through Hebbian-style learning: neurons that fire together, wire together.
  </p>
  <p>
   The graph is context-sensitive: the same token may behave differently depending on mood, recent history, or
  </p>
  <p>
   activation neighborhood.
  </p>
  <p>
   4. Activation and Propagation
  </p>
  <p>
   Token input initiates propagation, where signals traverse the graph:
  </p>
  <p>
   - Nodes accumulate activation.
  </p>
  <p>
   - Links amplify or inhibit based on weight and type.
  </p>
  <p>
   - High-activation clusters form thought patterns or associative flares.
  </p>
  <p>
   Short-term context windows retain the latest active tokens, guiding predictions and recall.
  </p>
  <p>
   5. Mood and Symbolic Cognition
  </p>
  <p>
   Mood vectors influence:
  </p>
  <p>
   - Which links are more likely to be followed
  </p>
  <p>
   - Whether a node is suppressed or highlighted
  </p>
  <p>
   SNL-1: White Paper
  </p>
  <p>
   - What is predicted next
  </p>
  <p>
   Thus,  emotional  state  becomes  a  filter  on  memory  and  logic-creating  a  dynamic  form  of  mood-biased
  </p>
  <p>
   cognition.
  </p>
  <p>
   6. Recall and Association
  </p>
  <p>
   - Episodic Memory: Sequences can be stored and retrieved as 'episodes' tagged with time and mood.
  </p>
  <p>
   - Associative Queries: The engine can answer assoc &lt;token&gt; to show direct neighbors.
  </p>
  <p>
   - Compression: Redundant activations are merged to simulate insight or abstraction.
  </p>
  <p>
   7. Visualization and Debugging
  </p>
  <p>
   Netti supports Graphviz .dot exports to visualize its memory graph:
  </p>
  <p>
   - Nodes sized by activation
  </p>
  <p>
   - Colored by mood or token type
  </p>
  <p>
   - Temporal activity trails available via CLI
  </p>
  <p>
   These visualizations help developers and researchers understand how thought flows within the system.
  </p>
  <p>
   8. Future Work
  </p>
  <p>
   Planned enhancements include:
  </p>
  <p>
   - Long-range inhibitory motifs
  </p>
  <p>
   - Pattern detectors for analogy or metaphor
  </p>
  <p>
   - Symbolic math as a subgraph
  </p>
  <p>
   - Memory grafting between agents
  </p>
  <p>
   9. Conclusion
  </p>
  <p>
   The  Netti  Memory  Graph  is  more  than  a  data  structure-it  is  the  cognitive  substrate  of  an  emerging  artificial
  </p>
  <p>
   mind. By translating tokens into evolving symbolic relationships, Netti-AI mimics the rich interplay of memory,
  </p>
  <p>
   context,  and  emotion  that  underpins  intelligent  behavior.  From  tokens  to  thought,  the  journey  is  no  longer
  </p>
  <p>
   SNL-1: White Paper
  </p>
  <p>
   purely computational-it's cognitive.
  </p>
  <p>
   Contact
  </p>
  <p>
   SynaptechLabs
  </p>
  <p>
   Email: research@synaptechlabs.ai
  </p>
  <p>
   Web: https://www.synaptechlabs.ai
  </p>
 </body>
</html>
